# Wrangling with Webpack - pt.2 - On the Subject of Memory

*Working Draft*

**Link to Article 1**

## Introduction

This is the second half of a two-part series on how we use Webpack here at Clio. Last time, I talked briefly about the problems we faced running Webpack on our CI provider before transitioning over to discussing our upgrade path to Webpack v4. The narrative then ended on a bit of a cliff-hanger as I noted that the upgrade did not immediately solve our problems, so I would like to amend that in this article. Welcome to Wrangling with Webpack 2 - :zap: Electric Boogaloo :zap:.

## Can You Repeat the Problem?

To set the stage, I would like to begin this article by reiterating the issue that was introduced in the last article.

We use Buildkite as our CI provider, and AWS EC2 instances to actually run our CI processes. When a developer pushes a new commit up to our Github repo, Buildkite is notified via webhooks and will create a new build for this commit in each of our CI pipelines.

We have multiple pipelines for doing all sorts of stuff, but the focus of this article is on the compile pipeline, which is used to automatically compile our front-end assets. When Buildkite creates a build, it will allocate an AWS EC2 instance and install an "agent" on this machine. The agent is basically a script that will begin running the processes defined in the pipeline on the machine it was installed on:

1. It clones the Github repo and checks out the commit that triggered the build
2. It performs a number of preparatory tasks such as installing Node modules
3. It compiles all of the assets in the Rails asset pipeline (mainly used for our legacy application)
4. It compiles our revamped application, Apollo, using Webpack

This pipeline worked great, until it didn't. Our AWS EC2 instances are of type `c5.large`, which comes with 4gb of memory. But because of other processes, by the time Webpack begins, there is only around 2gb of memory available for it to use. As the size of Apollo grew, Webpack started demanding more and more memory. Eventually, as Webpack was starved of the memory it needs, we started experiencing excruciatingly slow builds with intermittent failures.

Now that the problem has been clearly identified, let's talk about the ways we tried to deal with it.

## The Blackbox Approach

I would like to introduce the first approach we took as the "blackbox" approach, in reference to the blackbox testing method that many programmers are familiar with. In this approach, we treated our Webpack project as a "black box" that can only be examined and controlled from the outside.

No matter how complex your Webpack project may be, Webpack is still just a Node program that must conform to all of the normal rules that apply to any other Node program.

This means two things. First, when Webpack is running, you should be able to examine it using the `ps` utility. Second, because it is a Node program, you should be able to configure the Node process that runs it using standard CLI options, such as `--max-old-space-size`.

Let me show you how we used these tools.

To begin our analysis, we took the command that we use to compile Apollo on CI:
```
NODE_ENV=production webpack --progress --config ./config/webpack/production.js
```

And stuck it into our `package.json`:
```JSON
{
  "scripts": {
    "build:webpack": "NODE_ENV=production webpack --progress --config ./config/webpack/production.js"    
  }
}
```

This allowed us run our Webpack compilation by simply typing `yarn build:webpack`.

With Webpack running, we opened up another terminal window and executed:
```
ps ufaxww
```

`ps` is a Unix utility for printing out a snapshot of all active processes. By passing `ufaxww`, the output will show you the details of all active processes and also group together parent-child processes.

For example, here is the output we saw when we ran this command, truncated to focus on Webpack-related processes:

```sh
vagrant   3617  0.8  1.0 1138236 64720 pts/0   Sl+  17:45   0:00  node /usr/local/bin/yarn build:webpack
vagrant   3627  0.0  0.0   4300   772 pts/0    S+   17:45   0:00   \_ /bin/sh -c NODE_ENV=production node ./node_modules/webpack/bin/webpack.js --progress --config ./config/webpack/production.js
vagrant   3628 68.7  9.7 1700668 599088 pts/0  Rl+  17:45   0:29       \_ node ./node_modules/webpack/bin/webpack.js --progress --config ./config/webpack/production.js
vagrant   3634 27.5  5.6 1151908 345332 pts/0  Sl+  17:45   0:11           \_ /usr/local/stow/nodejs-8.9.4/bin/node /home/vagrant/clio/webpack/themis/node_modules/fork-ts-checker-webpack-plugin/lib/service.js
```

The sixth column from the left represents the amount of physical memory (in kb) being consumed by the process on that line. The first two lines are actually generated by `yarn`, which we were using to execute the `build:webpack` command. The third line is the actual Webpack process. The forth line is a parallel process spawned by a plugin called `fork-ts-checker-webpack-plugin` which we use to try and speed up our build. All in all, Webpack was consuming roughly 944mb of memory when this snapshot was taken.

Now that we had a sense of what is actually running when Webpack begins, we got rid of the extraneous output and asked for a summary statistic instead:
```
ps uax | grep webpack | awk '{s+=$6} END {print s}
```

This command will filter down the output from `ps` to only lines that contain the string "webpack". It then adds together the sixth column from each row and prints it out. In effect, running this command gave us a snapshot of how much memory Webpack was using in total. Depending on your situation, you may find it necessary to use a different filter, but for us, `grep webpack` was sufficient.

But a single snapshot is not all that useful. So we made a Node script:

```js
// profile-memory.js
var process = require("process");
var childProcess = require("child_process");

var repeating = setInterval(
  function() {
    childProcess.exec(
      "ps uax | grep webpack | awk '{s+=$6} END {print s}'",
      function(_err, stdout) {
        process.stdout.write(stdout.trim() + ",\n");
      }
    )
  },
  100
);

const webpack = childProcess.spawn("yarn", ["build:webpack"])
  .on("close", () => {
    clearInterval(repeating);
  });

webpack.stderr.on("data", (data) => {
  process.stderr.write(data);
});
```

When executed, `profile-memory.js` starts a new Webpack build. Then, as the build runs, a function is executed every 10th of a second that grabs the total memory being used and writes the result to standard output. Conveniently, the output of the script has been designed to be easily consumable by graphing applications.

We ran the script three times and uploaded the files to [plot.ly](https://plot.ly) to create this graph. :chart_with_upwards_trend:

![Webpack Memory Plot Baseline](assets/plot-baseline.png)
https://plot.ly/~XiaoChenClio/16/

As you can see, with our current configuration, Webpack finishes compilation in around 120 ~ 130s. At the very most, our Webpack compilation appears to use about 2 million kilobytes (2gb) of memory.

Now that we had an idea of what our Webpack build costs, what could we do? Well, this is where some knowledge about Node came in handy. Node uses V8 and can accept a number of command line options that configure the way V8 behaves. One of these is `--max-old-space-size`. To summarize Irina's [post](https://medium.com/@_lrlna/garbage-collection-in-v8-an-illustrated-guide-d24a952ee3b8), V8's memory management strategy divides up the heap into two parts: old space and new space. Objects in the old space are not subject to the frequent scavenging that occurs in the new space and are only garbage-collected when its capacity is reached. Therefore, if you set a lower capacity on the old space than the V8 default, it should lead to more garbage collections, albeit at the cost of total runtime. `--max-old-space-size` accepts a single number, interpreted as the maximum size of the old space in megabytes.

So, with this knowledge, we changed our `build:webpack` command so that we can control the size of the old space:

```
"build:webpack": "NODE_ENV=production node --max-old-space-size=SIZE ./node_modules/webpack/bin/webpack.js --progress --config ./config/webpack/production.js",
```

We tried three different values for `SIZE`: 1024, 896 and 768. Our project compiled successfully at both 1024 and 896, but failed at 768. Using `profile-memory.js`, the results of the two successful trials were logged. We created a graph to compare the results:

![Webpack Memory Plot Old Space](assets/plot-old-space.png)
https://plot.ly/~XiaoChenClio/1/

As we reduced the maximum size of the old space, Webpack took longer to build our project, but the amount of memory it consumed also decreased. We found `--max-old-space-size=1024` to be a sweet spot, as it reduced the the maximum memory used by around 400mb, at acceptable cost of +10s in total compile time.

It is also worth noting `--max-old-space-size` will have no effect on parallel processes spawned during compilation, such as `fork-ts-checker`. It only affects the immediate Node process it is passed to.

### Key takeaways

1. Good tooling is essential to solving any problem. Without it, you can't properly diagnose a problem nor can you validate different hypotheses
2. Node let's you control the size of V8's old space; setting it to a lower value can help reduce the amount of memory Webpack takes to compile your project, at the cost of longer compile times.

## The Whitebox Approach

Admittedly, the effectiveness of the methods described in blackbox approach is limited:

1. While `profile-memory.js` gives you a good high-level overview of your Webpack project, it does not tell you exactly where the memory usage is coming from
2. While limiting old max space size may help reduce memory usage, it comes at the cost of longer runtimes and also have to be adjusted and increased over time as your project grows

In short, thus far, we have only been able to identify the issue and find ways to mitigate it, as opposed to actually finding the root causes and addressing the problem directly.

In order to do that, we must shine light on the box and look inside of it.
